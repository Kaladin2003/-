{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caeea3bd-0236-4d4a-a550-fbe18462ac45",
   "metadata": {},
   "source": [
    "## Βήμα 1,2.  Συλλογή δεδομένων και Προεπεξεργασία κειμένου (Text Processing): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306cf2f-69e3-445a-a31b-b6e749813bc4",
   "metadata": {},
   "source": [
    "### libraries του κώδικα :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae367779-1faf-413d-8fdf-dd68473aa0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(word.lower() for word in stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c071695-e2f1-4ecd-8d1e-8d9c70ab5fdd",
   "metadata": {},
   "source": [
    "### Text Processing ΑΛΓΌΡΙΘΜΟΣ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76150fef-4de8-4fe8-a3ad-5713d8ab69b3",
   "metadata": {},
   "source": [
    "Το βήμα αυτό εστιάζει στην προεπεξεργασία του συνόλου δεδομένων που ανακτήσαμε από τις ιστοσελίδες στο πρώτο βήμα. Η προεπεξεργασία περιέχει πεζοποίηση των χαρακτήρων, tokenization, δηλαδή διάσπαση του κειμένου σε μεμονωμένους όρους, stemming/lemmatization, δηλαδή αποκοπή των καταλήξεων από κάθε λέξη και προσδιορισμό της κάθε μιας από το λήμμα της, αφαίρεση stop-word, δηλαδή λέξεων που είναι αρκετά κοινές και αφαίρεση ειδικών χαρακτήρων όπως σημείων στίξης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7706e92-0e88-4284-be32-b66a25a1b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#για το Text Processing \n",
    "def tokenized_text(text):   \n",
    "    tokens = nltk.word_tokenize(text)  # Tokenization\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]  # Αφαίρεση stop word\n",
    "    tokens = [word for word in tokens if word not in string.punctuation] # Αφαίρεση σημείων στίξης\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens] #Lemmatization\n",
    "    unique_tokens = list(set(tokens))\n",
    "    return unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d8323-ca84-47e2-b819-ef6344f043a7",
   "metadata": {},
   "source": [
    "### WEB CRAWLER ΑΛΓΌΡΙΘΜΟΣ ΚΑΙ ΑΠΟΘΗΚΕΥΣΗ ΣΤΟ JASON :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e301eb-25cb-475e-84c7-fe89a7527414",
   "metadata": {},
   "source": [
    "Το σύνολο δεδομένων που ανακτάται προκύπτει μέσω Web crawler που υλοποιήσαμε με την βοηθεια της BeautifulSoup. Η συλλογή των δεδομένων ξεκινάει από ένα URL της Wikipedia και στη συνέχεια το crawler επισκέπτεται τα υπόλοιπα URL που θα αντικρίσει στο αρχικό link. Ο αριθμός των links που θα επισκεφθεί καθορίζεται από την μεταβλητή max_urls. Για την αποθήκευση των δεδομένων σε μορφή Json υλοποιήσαμε αντίστοιχη συνάρτηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03eba1fa-9cec-4161-a615-b7a5f404daff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Δώστε το URL της σελίδας Wikipedia:  https://en.wikipedia.org/wiki/PC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://en.wikipedia.org/wiki/PC\n",
      "Scraping: https://en.wikipedia.org/wiki/Main_Page\n",
      "Scraping: https://en.wikipedia.org/wiki/Wikipedia\n",
      "Scraping: https://en.wikipedia.org/wiki/English_Wikipedia\n",
      "Scraping: https://en.wikipedia.org/wiki/Home_page\n",
      "Scraping: https://en.wikipedia.org/wiki/Home_screen\n",
      "Scraping: https://en.wikipedia.org/wiki/Smartphone_patent_wars\n",
      "Scraping: https://en.wikipedia.org/wiki/Smartphone\n",
      "Scraping: https://en.wikipedia.org/wiki/Smartphone_(disambiguation)\n",
      "Scraping: https://en.wikipedia.org/wiki/This_Man_(Cory_Marks_album)\n",
      "Scraping: https://en.wikipedia.org/wiki/This_Man_(album)\n",
      "Scraping: https://en.wikipedia.org/wiki/This_Man_(disambiguation)\n",
      "Scraping: https://en.wikipedia.org/wiki/This_Man\n",
      "Scraping: https://en.wikipedia.org/wiki/Conceptual_art\n",
      "Scraping: https://en.wikipedia.org/wiki/Concept_art\n",
      "Αποθήκευση σε JSON: wikipedia_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#wed crawler algo\n",
    "def scrape_wikipedia(start_url, max_urls=40, visited=None, scr_c=0):\n",
    "    if visited is None:\n",
    "        visited = set() \n",
    "\n",
    "    if start_url in visited or scr_c >= max_urls:\n",
    "        return [], scr_c\n",
    "\n",
    "    visited.add(start_url)\n",
    "    print(f\"Scraping: {start_url}\")\n",
    "    url_open = requests.get(start_url)\n",
    "    soup = BeautifulSoup(url_open.content, 'html.parser')\n",
    "    page_info = []\n",
    "\n",
    "    # Βρίσκει τον τίτλο της σελίδας\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}) \n",
    "    if title:\n",
    "        title = title.text.strip()\n",
    "    else:\n",
    "        title = \"No Title Found\"\n",
    "\n",
    "    # Παίρνει όλο το κείμενο \n",
    "    content = soup.find_all(['p', 'li'])  \n",
    "    full_text = \" \".join([element.text.strip() for element in content])\n",
    "\n",
    "    # tokenize το πλήρες κείμενο\n",
    "    processed_text = tokenized_text(full_text)\n",
    "\n",
    "    # Αποθήκευση δεδομένων της σελίδας\n",
    "    page_info.append({\n",
    "        \"url\": start_url,\n",
    "        \"title\": title,\n",
    "        \"full_text\": full_text,  # Αποθηκεύει το πλήρες κείμενο\n",
    "        \"processed_text\": processed_text\n",
    "    })\n",
    "\n",
    "    scr_c += 1  # Αύξηση μετρητή scrape για το πόσα έχει κανει scrape \n",
    "\n",
    "    # Βρείτε και scrape άλλους συνδέσμους Wikipedia\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if scr_c >= max_urls:\n",
    "            break\n",
    "\n",
    "        href = link['href']\n",
    "        if href.startswith(\"/wiki/\") and not re.search(r':', href):  #για να αγνοεί τα special links\n",
    "            full_url = \"https://en.wikipedia.org\" + href\n",
    "            if full_url not in visited:\n",
    "                additional_data, scr_c = scrape_wikipedia(\n",
    "                    full_url, max_urls, visited, scr_c\n",
    "                )\n",
    "                page_info.extend(additional_data)\n",
    "\n",
    "    return page_info, scr_c\n",
    "\n",
    "# για το βημα 1ο η αποθήκεση στο jason \n",
    "def save_to_json(file_name, data):\n",
    "    with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Αποθήκευση σε JSON: {file_name}\")\n",
    "\n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "    start_url = input(\"Δώστε το URL της σελίδας Wikipedia: \")\n",
    "    max_urls = 15\n",
    "\n",
    "    # Βήμα 1: Συλλογή δεδομένων\n",
    "    scraped_data, _ = scrape_wikipedia(start_url, max_urls)\n",
    "    save_to_json(\"wikipedia_data.json\", scraped_data)  # Αποθήκευση δεδομένων σε json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00464ad9-0153-44bc-8881-43180a999b69",
   "metadata": {},
   "source": [
    "urls που έχει επισκεφτεί το web crawler  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd45e7-4c33-42f8-8c52-a1600b2152b8",
   "metadata": {},
   "source": [
    "## Βήμα 3. Ευρετήριο (Indexing): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c07c7-2eb0-4b75-ab7e-c451f2578d10",
   "metadata": {},
   "source": [
    "Το ανεστραμμένο ευρετήριο είναι μια δομή δεδομένων που καταχωρεί για κάθε λέξη τα έγγραφα στα οποία αυτή η λέξη εμφανίζεται. Η διαδικασία αυτή πραγματοποιείται μέσω της συλλογής των tokens κάθε εγγράφου και της καταχώρησης των εγγράφων που περιέχουν κάθε λέξη στο ευρετήριο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5ee73c-9380-4c26-827d-b11d9c05eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποθήκευση σε JSON: inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "#για το Indexing\n",
    "\n",
    "def inverted_index(data):\n",
    "    inv_i = {} # Αρχικοποίηση του ανεστραμμένου ευρετηρίο\n",
    "\n",
    "    for i, page in enumerate(data):\n",
    "        doc_id = i + 1 # για να βάλουμε αριθμό σε κάθε έγγραφο \n",
    "        for word in page[\"processed_text\"]:\n",
    "            if word not in inv_i:\n",
    "                inv_i[word] = set()\n",
    "            inv_i[word].add(doc_id)\n",
    "\n",
    "    # Μετατροπή συνόλων σε λίστες μετά τη δημιουργία του ανεστραμμένου\n",
    "    for word in inv_i:\n",
    "        inv_i[word] = list(inv_i[word])\n",
    "\n",
    "    return inv_i\n",
    "\n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "    # Βήμα 3: Ευρετήριο\n",
    "    inv_i = inverted_index(scraped_data)\n",
    "    save_to_json(\"inverted_index.json\", inv_i)  # Αποθήκευση του inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647e66e-6a67-4584-b047-37aef8ce4802",
   "metadata": {},
   "source": [
    "## Βήμα 4. Μηχανή αναζήτησης (Search Engine):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567d295-8895-490b-9271-11674565a169",
   "metadata": {},
   "source": [
    "## Βήμα 4α Επεξεργασία ερωτήματος (Query Processing): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c7c9f-82d2-472c-beb0-6a1ea2896c14",
   "metadata": {},
   "source": [
    "Το βήμα αυτό αποσκοπεί στην ανάπτυξη μεθόδου η οποία αφού λάβει και αναλύσει ένα ερώτημα του χρήστη, θα ανακτά έγγραφα σχετικά με το ερώτημα μέσω του ανεστραμμένου ευρετηρίου. Η αναζήτηση εγγράφων γίνεται με παράθεση ενός ή περισσότερων όρων(λέξεων), οι οποίοι πρώτα γίνονται tokenized, είτε μεμονωμένα είτε με χρήση Boolean τελεστών(and, or, not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ae1b46-cd71-4f1f-962e-5b6e25affd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Δώστε τους όρους της αναζήτησής σας:  pc and war\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved (Boolean Retrieval): {1, 2, 3, 4, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "#για το boolean retrieval του 4α \n",
    "def boolean_retrieval(query_tokens, inv_i):\n",
    "    docs = set() # τα έγγραφα που έχουμε πάρει \n",
    "    all_docs = set(doc for postings in inv_i.values() for doc in postings)\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            results.append(token)\n",
    "        else:\n",
    "            preprocessed_tokens = tokenized_text(token)   # επεξεργασία όρων του query\n",
    "            if preprocessed_tokens:\n",
    "                results.append(preprocessed_tokens[0])\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        token = results[i]\n",
    "        if token == \"and\":\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs & doc_with_term if docs else doc_with_term\n",
    "        elif token == \"or\":\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs | doc_with_term if docs else doc_with_term\n",
    "        elif token == \"not\":\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs - doc_with_term if docs else all_docs - doc_with_term \n",
    "        else:  # απλός όρος\n",
    "            term = token\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs | doc_with_term\n",
    "    return docs   \n",
    "\n",
    "\n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "       # Βήμα 4: Επεξεργασία ερωτήματος και κατάταξη αποτελεσμάτων\n",
    "    query = input(\"Δώστε τους όρους της αναζήτησής σας: \")\n",
    "    # Boolean Retrieval\n",
    "    retrieved_docs = boolean_retrieval(tokenized_text(query), inv_i)\n",
    "    print(f\"Retrieved (Boolean Retrieval): {retrieved_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808189df-fc6a-4a64-800e-9e3d769d585b",
   "metadata": {},
   "source": [
    "## Βήμα 4β Κατάταξη αποτελεσμάτων (Ranking): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ccec2-b295-49f4-a0c7-3fe8122bceea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f609964-dbf7-45fd-abac-2969eeb49e22",
   "metadata": {},
   "source": [
    "### BOOLEAN RANKING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99f90f-787b-45c0-aab4-97f192aa4b08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cae68183-4079-46fa-93d8-01ee3808a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Δώστε τους όρους της αναζήτησής σας:  pc or war\n",
      "Επιλέξτε μέθοδο ανάκτησης(boolean/vsm):  boolean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved (Boolean Retrieval): {1, 2, 3, 4, 7, 8}\n",
      "\n",
      "κατάταξη αποτελεσμάτων (Boolean Ranking):\n",
      "1. Document 8 (Score: 0.0456)\n",
      "   Title: Smartphone\n",
      "   URL: https://en.wikipedia.org/wiki/Smartphone\n",
      "2. Document 1 (Score: 0.0359)\n",
      "   Title: PC\n",
      "   URL: https://en.wikipedia.org/wiki/PC\n",
      "3. Document 2 (Score: 0.0357)\n",
      "   Title: Main Page\n",
      "   URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "4. Document 7 (Score: 0.0345)\n",
      "   Title: Smartphone patent wars\n",
      "   URL: https://en.wikipedia.org/wiki/Smartphone_patent_wars\n",
      "5. Document 4 (Score: 0.0226)\n",
      "   Title: English Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/English_Wikipedia\n",
      "6. Document 3 (Score: 0.0131)\n",
      "   Title: Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/Wikipedia\n"
     ]
    }
   ],
   "source": [
    "#για το  Vector Space Model\n",
    "def alg_vector_space_Model(vector_a, vector_b):\n",
    "    product = np.dot(vector_a, vector_b) # Υπολογισμός εσωτερικού γινομένου\n",
    "    mag = np.linalg.norm(vector_a) * np.linalg.norm(vector_b) # Υπολογισμός μέτρου\n",
    "    return product / mag if mag else 0 # Επιστροφή συσχέτισης \n",
    "\n",
    "\n",
    "#για το tfidf \n",
    "def alg_tfidf_pin(data):\n",
    "    corpus = [\" \".join(page[\"processed_text\"]) for page in data]  # Δημιουργία κειμένου \n",
    "    vectorizer = TfidfVectorizer()  # Αρχικοποίηση \n",
    "    tfidf_pin = vectorizer.fit_transform(corpus)  # Εφαρμογή του TF-IDF \n",
    "    feature_names = vectorizer.get_feature_names_out()  # Λήψη των χαρακτηριστικών\n",
    "    return tfidf_pin, feature_names, vectorizer\n",
    "\n",
    "\n",
    "def alg_tfidf_ranking(query, tfidf_pin, characters, vector):\n",
    "    query_vector = vector.transform([\" \".join(tokenized_text(query))])  \n",
    "    scores = (tfidf_pin @ query_vector.T).toarray().flatten()  # Υπολογισμός βαθμολογίας για το ποσο ίδια είναι\n",
    "    ranked_docs = np.argsort(-scores) #ταξινόμηση σε  φθίνουσα σειρά\n",
    "    return ranked_docs, scores\n",
    "\n",
    "#Για το boolean ranking\n",
    "\n",
    "def alg_boolean_ranking(query_tokens, inv_i, tfidf_pin, characters, vector):\n",
    "    # Αρχικοποίηση ενός κενού συνόλου που θα περιέχει τα έγγραφα που ταιριάζουν με το ερώτημα.\n",
    "    docs = set()  \n",
    "\n",
    "    # Δημιουργία συνόλου με όλα τα διαθέσιμα έγγραφα από το ανεστραμμένο ευρετήριο.\n",
    "    all_docs = set(doc for postings in inv_i.values() for doc in postings)\n",
    "\n",
    "    # Λίστα που θα περιέχει τους όρους του ερωτήματος και τους λογικούς τελεστές.\n",
    "    results = []\n",
    "\n",
    "    # Λεξικό για να αποθηκευτούν οι βαθμολογίες των εγγράφων.\n",
    "    doc_scores = {}\n",
    "\n",
    "    # **Ανάλυση του ερωτήματος και εφαρμογή λογικών πράξεων**\n",
    "    for token in query_tokens:\n",
    "        # Αν ο όρος είναι λογικός τελεστής (\"and\", \"or\", \"not\"), τον προσθέτουμε ως έχει στη λίστα.\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            results.append(token)\n",
    "        else:\n",
    "            # Επεξεργασία των όρων του ερωτήματος (lemmatization, αφαίρεση stop words κ.λπ.).\n",
    "            preprocessed_tokens = tokenized_text(token)\n",
    "            if preprocessed_tokens:\n",
    "                # Προσθέτουμε τον επεξεργασμένο όρο στη λίστα.\n",
    "                results.append(preprocessed_tokens[0])\n",
    "\n",
    "    # λογικές πράξεις \n",
    "    for i in range(len(results)):\n",
    "        token = results[i]\n",
    "        if token == \"and\":  #  \"AND\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            # Βρίσκουμε τα έγγραφα που εχουν τον όρο\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            \n",
    "            docs = docs & doc_with_term if docs else doc_with_term\n",
    "        elif token == \"or\":  # \"OR\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            # Επιστρέφουμε την ένωση των εγγράφων που έχουν συλλεχθεί με αυτά που περιέχουν τον όρο.\n",
    "            docs = docs | doc_with_term if docs else doc_with_term\n",
    "        elif token == \"not\":  # Λογικός τελεστής \"NOT\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            # Αφαιρούμε από τα έγγραφα αυτά που περιέχουν τον όρο\n",
    "            docs = docs - doc_with_term if docs else all_docs - doc_with_term\n",
    "        else:  # Απλός όρος \n",
    "            term = token\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs | doc_with_term\n",
    "\n",
    "    #Υπολογισμός βαθμολογιών \n",
    "    for doc_id in docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            # Ελέγχουμε αν ο όρος υπάρχει \n",
    "            if term in characters:\n",
    "                term_index = characters.tolist().index(term)  # Εντοπισμός του όρου\n",
    "                # Προσθέτουμε τη βαθμολογία\n",
    "                score += tfidf_pin[doc_id - 1, term_index]  \n",
    "        doc_scores[doc_id] = score\n",
    "\n",
    "    # ταξινόμηση\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranked_doc_ids = [doc_id for doc_id, _ in ranked_docs]\n",
    "    return ranked_doc_ids, doc_scores\n",
    "\n",
    "\n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    tfidf_pin, feature_names, vectorizer = alg_tfidf_pin(scraped_data)\n",
    "    \n",
    "    # Βήμα 4: Επεξεργασία ερωτήματος και κατάταξη αποτελεσμάτων\n",
    "    query = input(\"Δώστε τους όρους της αναζήτησής σας: \")\n",
    "    algo_choice = input(\"Επιλέξτε μέθοδο ανάκτησης(boolean/vsm): \").strip().lower()\n",
    "    # Boolean Ranking\n",
    "    if algo_choice == \"boolean\":\n",
    "        \n",
    "        # Boolean Retrieval\n",
    "        retrieved_docs = boolean_retrieval(tokenized_text(query), inv_i)\n",
    "        print(f\"Retrieved (Boolean Retrieval): {retrieved_docs}\")\n",
    "        ranked_docs, doc_scores = alg_boolean_ranking(tokenized_text(query), inv_i, tfidf_pin, feature_names, vectorizer)\n",
    "        print(\"\\nκατάταξη αποτελεσμάτων (Boolean Ranking):\")\n",
    "        for rank, doc_id in enumerate(ranked_docs[:15], start=1):  \n",
    "            print(f\"{rank}. Document {doc_id} (Score: {doc_scores[doc_id]:.4f})\")\n",
    "            print(f\"   Title: {scraped_data[doc_id - 1]['title']}\")\n",
    "            print(f\"   URL: {scraped_data[doc_id - 1]['url']}\")\n",
    "     # VSM/TF-IDF Ranking\n",
    "    elif algo_choice == \"vsm\":\n",
    "        # VSM/TF-IDF Ranking\n",
    "        vsm_ranked_docs, scores = alg_tfidf_ranking(query, tfidf_pin, feature_names, vectorizer)\n",
    "        print(\"\\nκατάταξη αποτελεσμάτων (VSM):\")\n",
    "        for rank, doc_id in enumerate(vsm_ranked_docs[:15], start=1): \n",
    "            print(f\"{rank}. Document {doc_id + 1} (Score: {scores[doc_id]:.4f})\")\n",
    "            print(f\"   Title: {scraped_data[doc_id]['title']}\")\n",
    "            print(f\"   URL: {scraped_data[doc_id]['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75c4b-9efd-4a23-a4c4-3011bede6d54",
   "metadata": {},
   "source": [
    "Αποτέλεσμα boolean ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe862fd0-3284-4a32-936b-0f40e4660581",
   "metadata": {},
   "source": [
    "### VSM RANKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f14ae-fa3a-433a-864e-22cc062ddefe",
   "metadata": {},
   "source": [
    "Vector Space Model, στην οποία κάθε έγγραφο και το ερώτημα αναπαρίσταται ως διάνυσμα στον n-διάστατο χώρο λέξεων. Η αναπαράσταση αυτή χρησιμοποιεί την τιμή του TF-IDF για κάθε λέξη, η οποία συνυπολογίζει τη συχνότητα εμφάνισης της λέξης σε κάθε έγγραφο σε σχέση με τη γενική συχνότητά της σε όλα τα έγγραφα. Η κατάταξη των εγγράφων γίνεται με βάση την ομοιότητα μεταξύ του διανύσματος του ερωτήματος και των διανυσμάτων των εγγράφων, χρησιμοποιώντας το συντελεστή συσχέτισης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39441e31-4e89-478c-8170-447e3f1fbb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Δώστε τους όρους της αναζήτησής σας:  pc or war\n",
      "Επιλέξτε μέθοδο ανάκτησης(boolean/vsm):  vsm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "κατάταξη αποτελεσμάτων (VSM):\n",
      "1. Document 8 (Score: 0.0331)\n",
      "   Title: Smartphone\n",
      "   URL: https://en.wikipedia.org/wiki/Smartphone\n",
      "2. Document 1 (Score: 0.0265)\n",
      "   Title: PC\n",
      "   URL: https://en.wikipedia.org/wiki/PC\n",
      "3. Document 7 (Score: 0.0244)\n",
      "   Title: Smartphone patent wars\n",
      "   URL: https://en.wikipedia.org/wiki/Smartphone_patent_wars\n",
      "4. Document 2 (Score: 0.0241)\n",
      "   Title: Main Page\n",
      "   URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "5. Document 4 (Score: 0.0152)\n",
      "   Title: English Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/English_Wikipedia\n",
      "6. Document 3 (Score: 0.0092)\n",
      "   Title: Wikipedia\n",
      "   URL: https://en.wikipedia.org/wiki/Wikipedia\n",
      "7. Document 5 (Score: 0.0000)\n",
      "   Title: Home page\n",
      "   URL: https://en.wikipedia.org/wiki/Home_page\n",
      "8. Document 6 (Score: 0.0000)\n",
      "   Title: Home screen\n",
      "   URL: https://en.wikipedia.org/wiki/Home_screen\n",
      "9. Document 9 (Score: 0.0000)\n",
      "   Title: Smartphone (disambiguation)\n",
      "   URL: https://en.wikipedia.org/wiki/Smartphone_(disambiguation)\n",
      "10. Document 10 (Score: 0.0000)\n",
      "   Title: This Man (album)\n",
      "   URL: https://en.wikipedia.org/wiki/This_Man_(Cory_Marks_album)\n",
      "11. Document 11 (Score: 0.0000)\n",
      "   Title: This Man (album)\n",
      "   URL: https://en.wikipedia.org/wiki/This_Man_(album)\n",
      "12. Document 12 (Score: 0.0000)\n",
      "   Title: This Man (disambiguation)\n",
      "   URL: https://en.wikipedia.org/wiki/This_Man_(disambiguation)\n",
      "13. Document 13 (Score: 0.0000)\n",
      "   Title: This Man\n",
      "   URL: https://en.wikipedia.org/wiki/This_Man\n",
      "14. Document 14 (Score: 0.0000)\n",
      "   Title: Conceptual art\n",
      "   URL: https://en.wikipedia.org/wiki/Conceptual_art\n",
      "15. Document 15 (Score: 0.0000)\n",
      "   Title: Concept art\n",
      "   URL: https://en.wikipedia.org/wiki/Concept_art\n"
     ]
    }
   ],
   "source": [
    "#για το  Vector Space Model\n",
    "def alg_vector_space_Model(vector_a, vector_b):\n",
    "    product = np.dot(vector_a, vector_b) # Υπολογισμός εσωτερικού γινομένου\n",
    "    mag = np.linalg.norm(vector_a) * np.linalg.norm(vector_b) # Υπολογισμός μέτρου\n",
    "    return product / mag if mag else 0 # Επιστροφή συσχέτισης \n",
    "\n",
    "\n",
    "#για το tfidf \n",
    "def alg_tfidf_pin(data):\n",
    "    corpus = [\" \".join(page[\"processed_text\"]) for page in data]  # Δημιουργία κειμένου \n",
    "    vectorizer = TfidfVectorizer()  # Αρχικοποίηση \n",
    "    tfidf_pin = vectorizer.fit_transform(corpus)  # Εφαρμογή του TF-IDF \n",
    "    feature_names = vectorizer.get_feature_names_out()  # Λήψη των χαρακτηριστικών\n",
    "    return tfidf_pin, feature_names, vectorizer\n",
    "\n",
    "\n",
    "def alg_tfidf_ranking(query, tfidf_pin, characters, vector):\n",
    "    query_vector = vector.transform([\" \".join(tokenized_text(query))])  \n",
    "    scores = (tfidf_pin @ query_vector.T).toarray().flatten()  # Υπολογισμός βαθμολογίας για το ποσο ίδια είναι\n",
    "    ranked_docs = np.argsort(-scores) #ταξινόμηση σε  φθίνουσα σειρά\n",
    "    return ranked_docs, scores\n",
    "\n",
    "#Για το boolean ranking\n",
    "\n",
    "def alg_boolean_ranking(query_tokens, inv_i, tfidf_pin, characters, vector):\n",
    "    # Αρχικοποίηση ενός κενού συνόλου που θα περιέχει τα έγγραφα που ταιριάζουν με το ερώτημα.\n",
    "    docs = set()  \n",
    "\n",
    "    # Δημιουργία συνόλου με όλα τα διαθέσιμα έγγραφα από το ανεστραμμένο ευρετήριο.\n",
    "    all_docs = set(doc for postings in inv_i.values() for doc in postings)\n",
    "\n",
    "    # Λίστα που θα περιέχει τους όρους του ερωτήματος και τους λογικούς τελεστές.\n",
    "    results = []\n",
    "\n",
    "    # Λεξικό για να αποθηκευτούν οι βαθμολογίες των εγγράφων.\n",
    "    doc_scores = {}\n",
    "\n",
    "    # **Ανάλυση του ερωτήματος και εφαρμογή λογικών πράξεων**\n",
    "    for token in query_tokens:\n",
    "        # Αν ο όρος είναι λογικός τελεστής (\"and\", \"or\", \"not\"), τον προσθέτουμε ως έχει στη λίστα.\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            results.append(token)\n",
    "        else:\n",
    "            # Επεξεργασία των όρων του ερωτήματος (lemmatization, αφαίρεση stop words κ.λπ.).\n",
    "            preprocessed_tokens = tokenized_text(token)\n",
    "            if preprocessed_tokens:\n",
    "                # Προσθέτουμε τον επεξεργασμένο όρο στη λίστα.\n",
    "                results.append(preprocessed_tokens[0])\n",
    "\n",
    "    # λογικές πράξεις \n",
    "    for i in range(len(results)):\n",
    "        token = results[i]\n",
    "        if token == \"and\":  #  \"AND\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            # Βρίσκουμε τα έγγραφα που εχουν τον όρο\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            \n",
    "            docs = docs & doc_with_term if docs else doc_with_term\n",
    "        elif token == \"or\":  # \"OR\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            # Επιστρέφουμε την ένωση των εγγράφων που έχουν συλλεχθεί με αυτά που περιέχουν τον όρο.\n",
    "            docs = docs | doc_with_term if docs else doc_with_term\n",
    "        elif token == \"not\":  # Λογικός τελεστής \"NOT\".\n",
    "            i += 1\n",
    "            term = results[i]\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            # Αφαιρούμε από τα έγγραφα αυτά που περιέχουν τον όρο\n",
    "            docs = docs - doc_with_term if docs else all_docs - doc_with_term\n",
    "        else:  # Απλός όρος \n",
    "            term = token\n",
    "            doc_with_term = set(inv_i.get(term, []))\n",
    "            docs = docs | doc_with_term\n",
    "\n",
    "    #Υπολογισμός βαθμολογιών \n",
    "    for doc_id in docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            # Ελέγχουμε αν ο όρος υπάρχει \n",
    "            if term in characters:\n",
    "                term_index = characters.tolist().index(term)  # Εντοπισμός του όρου\n",
    "                # Προσθέτουμε τη βαθμολογία\n",
    "                score += tfidf_pin[doc_id - 1, term_index]  \n",
    "        doc_scores[doc_id] = score\n",
    "\n",
    "    # ταξινόμηση\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranked_doc_ids = [doc_id for doc_id, _ in ranked_docs]\n",
    "    return ranked_doc_ids, doc_scores\n",
    "\n",
    "\n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    tfidf_pin, feature_names, vectorizer = alg_tfidf_pin(scraped_data)\n",
    "    \n",
    "    # Βήμα 4: Επεξεργασία ερωτήματος και κατάταξη αποτελεσμάτων\n",
    "    query = input(\"Δώστε τους όρους της αναζήτησής σας: \")\n",
    "    algo_choice = input(\"Επιλέξτε μέθοδο ανάκτησης(boolean/vsm): \").strip().lower()\n",
    "    # Boolean Ranking\n",
    "    if algo_choice == \"boolean\":\n",
    "        \n",
    "        # Boolean Retrieval\n",
    "        retrieved_docs = boolean_retrieval(tokenized_text(query), inv_i)\n",
    "        print(f\"Retrieved (Boolean Retrieval): {retrieved_docs}\")\n",
    "        ranked_docs, doc_scores = alg_boolean_ranking(tokenized_text(query), inv_i, tfidf_pin, feature_names, vectorizer)\n",
    "        print(\"\\nκατάταξη αποτελεσμάτων (Boolean Ranking):\")\n",
    "        for rank, doc_id in enumerate(ranked_docs[:15], start=1):  \n",
    "            print(f\"{rank}. Document {doc_id} (Score: {doc_scores[doc_id]:.4f})\")\n",
    "            print(f\"   Title: {scraped_data[doc_id - 1]['title']}\")\n",
    "            print(f\"   URL: {scraped_data[doc_id - 1]['url']}\")\n",
    "     # VSM/TF-IDF Ranking\n",
    "    elif algo_choice == \"vsm\":\n",
    "        # VSM/TF-IDF Ranking\n",
    "        vsm_ranked_docs, scores = alg_tfidf_ranking(query, tfidf_pin, feature_names, vectorizer)\n",
    "        print(\"\\nκατάταξη αποτελεσμάτων (VSM):\")\n",
    "        for rank, doc_id in enumerate(vsm_ranked_docs[:15], start=1): \n",
    "            print(f\"{rank}. Document {doc_id + 1} (Score: {scores[doc_id]:.4f})\")\n",
    "            print(f\"   Title: {scraped_data[doc_id]['title']}\")\n",
    "            print(f\"   URL: {scraped_data[doc_id]['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442e9dd-9ffa-4bc5-859e-88b6a9b0b4e6",
   "metadata": {},
   "source": [
    "Αποτέλεσμα vsm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8d947-ad6b-4576-8a9d-6b3fda91df78",
   "metadata": {},
   "source": [
    "### Βήμα 5. Αξιολόγηση συστήματος: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990532ba-4485-444a-8753-e929f8373336",
   "metadata": {},
   "source": [
    "Το βήμα αυτό ασχολείται με την αξιολόγηση του συστήματος ανάκτησής μας. Οι αποδόσεις μετρούνται με ακρίβεια, ανάκτηση, f1-score και μέση ακρίβεια."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "807583b7-93f4-4878-bdd3-4c63823b0f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After Ranking: [[6, 5, 8, 1, 7, 4, 3], [4, 6, 3, 8, 7, 14], [6, 14, 4, 8, 7, 3]]\n",
      "Query 1: Precision = 0.14, Recall = 0.5, F1 = 0.22\n",
      "Query 2: Precision = 0.17, Recall = 0.5, F1 = 0.25\n",
      "Query 3: Precision = 0.17, Recall = 0.5, F1 = 0.25\n",
      "Mean Average Precision: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Βήμα 5ο Αξιολόγηση συστήματος\n",
    "    \n",
    "def evaluation(retrieved_docs, relevant_docs):\n",
    "    # Μετατροπή σε set\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "\n",
    "    true_positives = len(retrieved_set & relevant_set) # Τομή ανακτηθέντων με σχετικών εγγράφων\n",
    "    false_positives = len(retrieved_set - relevant_set) # Ανεκτηθέντα αλλά μη σχετικά\n",
    "    false_negatives = len(relevant_set - retrieved_set) # Σχετικά αλλά μη ανεκτηθέντα\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def MAP(retrieved, queries):\n",
    "    avg = []\n",
    "    for i, (query_text, relevant_docs) in enumerate(queries):\n",
    "        # Μετατροπή σε set\n",
    "        relevant_docs = set(relevant_docs)\n",
    "        retrieved_docs = set(retrieved[i])\n",
    "\n",
    "        precision_sum = 0\n",
    "        relevant_found = 0\n",
    "        # Υπολογισμός ακρίβειας \n",
    "        for rank, doc in enumerate(retrieved_docs, start = 1):\n",
    "            if doc in relevant_docs:\n",
    "                relevant_found += 1\n",
    "                precision_sum += relevant_found / rank\n",
    "        avg.append(precision_sum / len(relevant_docs) if relevant_docs else 0)\n",
    "    return sum(avg) / len(avg)      \n",
    "# MAIN \n",
    "if __name__ == \"__main__\":\n",
    "  # Βήμα 5: Αξιολόγηση συστήματος\n",
    "    retrieved = []\n",
    "    \n",
    "    queries = [\n",
    "        (\"pc and home\", [1, 3, 7, 8]),\n",
    "        (\"data or wiki\", [4, 3]),\n",
    "        (\"data not therefore\", [2, 4])\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for query_text, relevant_docs in queries:\n",
    "        #  κατάταξη αποτελεσμάτων με  Boolean retrieval και vsm \n",
    "        boolean_ranked_docs, _ = alg_boolean_ranking(tokenized_text(query_text), inv_i, tfidf_pin, feature_names, vectorizer)\n",
    "        vsm_ranked_docs, _ = alg_tfidf_ranking(query_text, tfidf_pin, feature_names, vectorizer)\n",
    "        #  κατάταξη αποτελεσμάτων με  Boolean retrieval\n",
    "        ranked_boolean_filtered = [doc_id for doc_id in boolean_ranked_docs if doc_id in vsm_ranked_docs]\n",
    "        retrieved.append(ranked_boolean_filtered)\n",
    "    \n",
    "    print(f\"\\n After Ranking: {retrieved}\")\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        precision, recall, f1 = evaluation(retrieved[i], relevant_docs) \n",
    "        print(f\"Query {i+1}: Precision = {precision:.2}, Recall = {recall:.2}, F1 = {f1:.2}\")\n",
    "    map_score = MAP(retrieved, queries)\n",
    "    print(f\"Mean Average Precision: {map_score:.2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46078fdd-1620-49b8-929e-7b14c1b271bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
